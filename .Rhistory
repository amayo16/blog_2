53 56 55 55
# do LOOCV: test each one against n - 1
library(MASS);
library(caret)
H4A.all <- lda(H4A[2:72],H4A$PopSex, data = H4a, prior = c(0.25,0.25,0.25,0.25), CV = T)
confusionMatrix(H4A$PopSex, H4A.all$class)
sprop = 0.3
holdout <- sample(nrow(H4A), round(sprop*nrow(H4A)))
H4A.a7 <- lda(H4A[2:72], H4A$PopSex, data = H4A, prior = c(0.25,0.25,0.25,0.25), subset =
holdout, CV = T)
Accuracies <- c(0.00)
for (i in seq(1000))
{
inTrain <- createDataPartition(y = H4A$PopSex, # y = grouping variable, stratified random split
## the outcome data are needed
p = .70, ## The proportion of records in the training set
list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
testrecs <- as.numeric(rownames(testing)) # row(testing)[,1]
H4A.R <- lda(H4A[inTrain,c(2:72)], H4A[inTrain,"PopSex"], data = HA4, prior =
c(0.25,0.25,0.25,0.25), subset = -inTrain, CV = T)
Accuracies[i] <- confusionMatrix(H4A[as.numeric(rownames(H4A.R$posterior)),"PopSex"],
H4A.R$
for (i in seq(1000))
{
inTrain <- createDataPartition(y = H4A$PopSex, # y = grouping variable, stratified random split
## the outcome data are needed
p = .70, ## The proportion of records in the training set
list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
testrecs <- as.numeric(rownames(testing)) # row(testing)[,1]
H4A.R <- lda(H4A[inTrain,c(2:72)], H4A[inTrain,"PopSex"], data = HA4, prior =
c(0.25,0.25,0.25,0.25), subset = -inTrain, CV = T)
Accuracies[i] <- confusionMatrix(H4A[as.numeric(rownames(H4A.R$posterior)),"PopSex"],
H4A.R$class)$overall["Accuracy"]
}
summary(Accuracies)
plot(density(Accuracies))
FourStep <- train(PopSex ~ ., data = training, maxvar = 10,
method = "stepLDA", improvement = 0.005,
trControl = trainControl(method = "boot632"))
inTrain <- createDataPartition(y = H4A$PopSex, # y = grouping variable
## the outcome data are needed
p = .80, ## The proportion of records in the training set
list = FALSE)
training <- H4A[ inTrain,]
testing <- H4A[-inTrain,]
nrow(training)
nrow(testing)
# caret uses other methods from packages such as klaR (lda, qda)to do things;
# the other packages have more documentaion on how to run them.
improvement: proportion of improvement: 0.01 = 1 % improvement
maxvar: maximum number of predictors
direct: direction of selection, "forward", "backward", "both" (default)
method: resampling- 'cv' (default k = 10); 'LOOCV', 'boot', 'boot632'
# default: both directions
FourStep <- train(PopSex ~ ., data = training, maxvar = 10,
method = "stepLDA", improvement = 0.005,
trControl = trainControl(method = "boot632"))
library(klaR)
H4A_swf <- stepclass(H4A[2:72], H4A$PopSex, "lda", improvement = 0.005, direction =
"forward", fold = 10)
H4A_swf
plot(H4A_swf)
Accuracies <- c(0.00)
#best model:
#PopSex ~ GOL + XCB + NLH + JUB + XML + FRS + FOL + BAA
for (i in seq(1000))
{
inTrain <- createDataPartition(y = H4A$PopSex, # y = grouping variable, stratified random split
## the outcome data are needed
p = .70, ## The proportion of records in the training set
list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
testrecs <- as.numeric(rownames(testing)) # row(testing)[,1]
H4A.RB <- lda(PopSex ~ GOL + XCB + NLH + JUB + XML + FRS + FOL + BAA, data = H4A, prior =
c(0.25,0.25,0.25,0.25), subset = -inTrain, CV = T)
Accuracies[i] <- confusionMatrix(H4A[as.numeric(rownames(H4A.RB$posterior)),"PopSex"],
H4A.RB$class)$overall["Accuracy"]
}
summary(Accuracies)
plot(density(Accuracies))
H4A_swfq <- stepclass(H4A[2:72], H4A$PopSex, "qda", improvement = 0.005,, fold = 5, maxvars = 15)
# multiple times
Accuracies <- c(0.00)
# best QDFA model:
# H4A$PopSex ~ GOL + XCB + OBH + XML + FOL + VRR
for (i in seq(1000))
{
inTrain <- createDataPartition(y = H4A$PopSex, # y = grouping variable, stratified random split
## the outcome data are needed
p = .70, ## The proportion of records in the training set
list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
# testrecs <- as.numeric(rownames(testing)) # row(testing)[,1]
H4A.RB <- qda(PopSex ~ GOL + XCB + OBH + XML + FOL + VRR, data = H4A, prior =
c(0.25,0.25,0.25,0.25), subset = -inTrain, CV = T)
Accuracies[i] <- confusionMatrix(H4A[as.numeric(rownames(H4A.RB$posterior)),"PopSex"],
H4A.RB$class)$overall["Accuracy"]
}
summary(Accuracies)
knn4 <- train(PopSex ~ ., data = training, method = "knn",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv")) update(knn4, list(.k = 3))
knn4 <- train(PopSex ~ ., data = training, method = "knn",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
knn4 <- train(PopSex ~ ., data = training, method = "knn",
tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
confusionMatrix(knn4)
knn4_pred <- predict(knn4,newdata = testing)
confusionMatrix(knn4_pred,testing$PopSex)
Accuracies <- c(0.00)
# best QDFA model:
# H4A$PopSex ~ GOL + XCB + OBH + XML + FOL + VRR
for (i in seq(20)) # only 20 befcause it takes a while
{
inTrain <- createDataPartition(y = H4A$PopSex, # y = grouping variable, stratified random split
## the outcome data are needed
p = .70, ## The proportion of records in the training set
list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
# testrecs <- as.numeric(rownames(testing)) # row(testing)[,1]
knn4 <- train(PopSex ~ ., data = training, method = "knn",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
plot(density(Accuracies))
####Blog Post 2#####
library(Lahman)
library(dplyr)
library(ggiraph)
library(ggplot2)
#Store the HR and SO data into a dataframe
df <- Batting%>%
group_by(playerID)%>%
summarize(Career_HR = sum(HR),Career_SO = sum(SO))%>%
filter(Career_HR >= 400)
#Join the dataframe with the Master table.  Bring in the first and last name of the player.
HR_vs_SO <- inner_join(df,Master,by=c("playerID"))%>%
select(nameFirst,nameLast,Career_HR,Career_SO)
#----------------------------------------------------
#Creating the scatterplot.
g<-ggplot()+
geom_point_interactive(data = HR_vs_SO, aes(x=Career_SO,y=Career_HR,tooltip=nameLast))+
ggtitle("Career Homeruns vs Strikeouts for Great Hitters")+
xlab("Career Strikouts")+
ylab("Career Homeruns")
ggiraph(code=print(g))
g
ggiraph(code=print(g))
HR_vs_SO$nameFirst
HR_vs_SO$nameLast
paste(HR_vs_SO$nameFirst,HR_vs_SO$nameLast)
HR_vs_SO$name <-paste(HR_vs_SO$nameFirst,HR_vs_SO$nameLast)
HR_vs_SO <- inner_join(df,Master,by=c("playerID"))%>%
select(name,Career_HR,Career_SO)
HR_vs_SO
g<-ggplot()+
geom_point_interactive(data = HR_vs_SO, aes(x=Career_SO,y=Career_HR,tooltip=name))+
ggtitle("Career Homeruns vs Strikeouts for Great Hitters")+
xlab("Career Strikouts")+
ylab("Career Homeruns")
ggiraph(code=print(g))
g<-ggplot()+
geom_point_interactive(data = HR_vs_SO, aes(x=Career_SO,y=Career_HR,tooltip=name,data_id=nameLast))+
ggtitle("Career Homeruns vs Strikeouts for Great Hitters")+
xlab("Career Strikouts")+
ylab("Career Homeruns")
ggiraph(code=print(g),hover_css="fill:white;stroke:black")
library(Lahman)
library(dply)
library(dplyr)
library(ggplot2)
library(ggiraph)
head(Master)
df <- Master%>%
select(weight)
df
ggplot()+
geom_histogram(data = df, aes(x=weight))
df <- Master%>%
filter(!is.na(weight))%>%
select(weight)
ggplot()+
geom_histogram(data = df, aes(x=weight))
ggplot()+
geom_histogram(data = df, aes(x=weight), color = "white")
ggplot()+
geom_histogram(data = df, aes(x=weight), color = "white", fill = "blue")
ggplot()+
geom_histogram(data = df, aes(x=weight), color = "white", fill = "blue")+
ggtitle("Baseball Player Weights")
ggplot()+
geom_histogram(data = df, aes(x=weight), color = "white", fill = "blue", bins = 10)+
ggtitle("Baseball Player Weights")
ggplot()+
geom_histogram(data = df, aes(x=weight), color = "white", fill = "blue", bins = 25)+
ggtitle("Baseball Player Weights")
ggplot()+
geom_histogram(data = df, aes(x=weight), color = "white", fill = "blue", bins = 30)+
ggtitle("Baseball Player Weights")
help(gglab)
help("ggplot")
df <- Teams%>%
filter(yearID == 1980)%>%
select(teamdID,HR)
###Barplot###
df <- Teams%>%
filter(yearID == 1980)%>%
select(teamID ,HR)
df
ggplot()+
geom_bar(data = df, aes(x = teamID, y = HR), stat = "identity")
ggplot()+
geom_bar(data = df, aes(x = teamID, y = HR), stat = "identity", color = "white", fill = "blue")
ggplot()+
geom_bar(data = df, aes(x = teamID, y = HR), stat = "identity", color = "blue", fill = "white")
head(Teams)
df <- Teams%>%
filter(yearID == 1980)%>%
select(name , HR)
ggplot()+
geom_bar(data = df, aes(x = names, y = HR), stat = "identity", color = "blue", fill = "white")
ggplot()+
geom_bar(data = df, aes(x = name, y = HR), stat = "identity", color = "blue", fill = "white")
ggplot()+
geom_bar(data = df, aes(x = HR, y = name), stat = "identity", color = "blue", fill = "white")  #ggplot does tallying for you.  Don't do any type of grouping because we already did it in the function.
ggplot()+
geom_bar(data = df, aes(x = name, y = HR), stat = "identity", color = "blue", fill = "white")+ #ggplot does tallying for you.  Don't do any type of grouping because we already did it in the function.
coord_flip()
df <- Teams%>%
filter(yearID == 1980)%>%
select(name , HR)%>%
arrange(HR)
ggplot()+
geom_bar(data = df, aes(x = name, y = HR), stat = "identity", color = "blue", fill = "white")+ #ggplot does tallying for you.  Don't do any type of grouping because we already did it in the function.
coord_flip()
df <- Teams%>%
filter(yearID == 1980)%>%
select(name , HR)%>%
arrange(HR)
ggplot()+
geom_bar(data = df, aes(x = name, y = HR), stat = "identity", color = "blue", fill = "white")+ #ggplot does tallying for you.  Don't do any type of grouping because we already did it in the function.
coord_flip()
df
df$name
str(df$name)
str(df$HR)
df$name <- factor(df$name)
str(df$name)
ggplot()+
geom_bar(data = df, aes(x = name, y = HR), stat = "identity", color = "blue", fill = "white")+ #ggplot does tallying for you.  Don't do any type of grouping because we already did it in the function.
coord_flip()
levels(df$name)
df$name <- factor(df$name, levels = df$name)
ggplot()+
geom_bar(data = df, aes(x = name, y = HR), stat = "identity", color = "blue", fill = "white")+ #ggplot does tallying for you.  Don't do any type of grouping because we already did it in the function.
coord_flip()
levels(df$name)
library(igraph)
library(graphTweets)
library(twitteR)
library(ROAuth)
library(httr)
library(plyr)
library(tm)
library(rpart)
library(rpart.plot)
library(zoo)
library(xts)
library(topicmodels)
library(topicmodels)
library(stringi)
library(LDAvis)
library(syuzhet)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
api_key <- "B4phyM9cP5I73JNWGWqfLsRCu"
api_secret <- "9NythHt3r1z10PHEquilHWg8eYlowHHzQIXLlGbmiQr5FfUTID"
access_token <- "2845584590-gp5fkyGlkBeZY9GQA4BfHwMqSDWRgYhm8Dkq06l"
access_token_secret <- "7IMDiZJLcoAJRRgj25H5D4YfKKoSdRiNPAGXmTXqi2y5z"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
tweets <- searchTwitter("Sinai", n=3000, lang="en")
tw_df <- twListToDF(tweets)
edges <- getEdges(data = tw_df, tweets = "text", source = "screenName", "retweetCount", str.length = 20)
nodes <- getNodes(edges, source = "source", target="target")
g <- graph.data.frame(edges, directed = TRUE, vertices = nodes)
write.graph(g, file="forgephi.graphml", format="graphml")
View(tw_df)
getwd()
# create dynamic graph and open in
Gephidyn <- dynamise(tw_df, tweets = "text", source = "screenName", start.stamp = "created", write = TRUE, open = TRUE)
write.graph(Gephidyn, file="dynamic2.graphml", format="graphml")
#### Can we add topic models ?
sk = tw_df$text
TextPreprocessing = lapply(sk, function(x) {
x = gsub('http\\S+\\s*', '', x) ## Remove URLs
x = gsub('\\b+RT', '', x) ## Remove RT
x = gsub('#\\S+', '', x) ## Remove Hashtags
x = gsub('@\\S+', '', x) ## Remove Mentions
x = gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
x = gsub("\\d", '', x) ## Remove Controls and special characters
x = gsub('[[:punct:]]', '', x) ## Remove Punctuations
x = gsub("^[[:space:]]*","",x) ## Remove leading whitespaces
x = gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces
})
# or as a vector
bd_list = as.vector(TextPreprocessing)
mycorpus <- Corpus(VectorSource(bd_list))
mycorpus = tm_map(mycorpus, content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')))
### Transformer all characters to lower case
mycorpus = tm_map(mycorpus, content_transformer(tolower))
### Remove all Punctuation
mycorpus = tm_map(mycorpus, removePunctuation)
### Remove all Numbers
mycorpus = tm_map(mycorpus, removeNumbers)
### Remove Stopwords
mycorpus = tm_map(mycorpus, removeWords, stopwords('english'))
#### transform to Document Term Matrix
skip.dtm = DocumentTermMatrix(mycorpus)
### Topic Model Analysis
rowTotals = apply(skip.dtm, 1, sum)
smtpmodel = skip.dtm[rowTotals>0, ]
smmodel_tweets = LDA(smtpmodel, 5)
terms(smmodel_tweets , 40)
### Preparation for Tableau
## Create dataframe of discovered topics
topic_col=topics(smmodel_tweets)
topic_col2=as.data.frame(topic_col)
### ONLY IF NEEDED (if rows are missing) Empty rows identifier for Topic Model JSON Application
empty.rows <- skip.dtm[rowTotals == 0, ]$dimnames[1][[1]]
## Find out the missing rows
empty.rows
## Remove empty row identified as "951" from tw_df dataframe
## For your analysis you may have more than one missing row.
## If you have more than one missing row, the code should look
## as follows
## For example   new.dtm.df = tw_df[-c(951, 224, 301, 501), ]
##Remember if you are not missing any rows, or rather your rows match
## for topic_col2   and tw_df   then you do not need to do this operation
## Just combine the two dataframes using line 144
new.dtm.df = tw_df[-c(221,222), ]
ix = which(rownames(tw_df) %in% c(empty.rows))
clean = tw_df[-ix, ]
date_tab = clean$created
### Adding Topic Models to the Total dataframe
new_tw_df = cbind(date_tab, topic_col2)
str(clean)
head(new_tw_df)
clean_sent = clean
### Add to the total dataframe and then the social network graph
edges <- getEdges(data = new_tw_df, tweets = "text", source = "screenName", "retweetCount", "favorited", "topic_col", str.length = 20)
nodes <- getNodes(edges)
g <- graph.data.frame(edges, directed = TRUE, vertices = nodes)
write.graph(g, "gephi_topics2.graphml", format="graphml")
## Prepare for sentiment analysis
library(syuzhet)
tw_df$text <- iconv(tw_df$text, 'UTF-8', 'ASCII')
# Get Sentiment Analysis
thaad_Sentiment = get_nrc_sentiment(tw_df$text)
#### Visualization for Overall Sentiment for Thaad related tweets
sentTotals = data.frame(colSums(thaad_Sentiment))
names(sentTotals) = "count"
sentTotals = cbind("sentiment" = rownames(sentTotals), sentTotals)
rownames(sentTotals) = NULL
ggplot(data = sentTotals, aes(x = sentiment, y = count)) +
geom_bar(aes(fill = sentiment), stat = "identity") +
theme(legend.position = "none") +
xlab("Sentiment") + ylab("Total Count") + ggtitle("Total Sentiment Score for Thaad Tweets")
ix = which(rownames(thaad_Sentiment) %in% c(empty.rows))
clean_sent = thaad_Sentiment[-ix, ]
new_tw_df_2 = cbind(clean_sent, new_tw_df)
## Write to CSV for Tableau
write.table(new_tw_df_2, "Coptic_Christians.csv", sep=",", col.names=T)
getwd()
## Write to CSV for Tableau
write.table(new_tw_df_2, "Sinai.csv", sep=",", col.names=T)
setwd("C:/Users/Andrew/Desktop/Social Media Anlysis")
## Write to CSV for Tableau
write.table(new_tw_df_2, "Sinai.csv", sep=",", col.names=T)
edges <- getEdges(data = new_tw_df, tweets = "text", source = "screenName", "retweetCount", "favorited", "topic_col", str.length = 20)
nodes <- getNodes(edges)
g <- graph.data.frame(edges, directed = TRUE, vertices = nodes)
write.graph(g, "gephi_sinai.graphml", format="graphml")
library(caret)
help(sda)
help(caret)
library(class) #k-nearest neighbors
library(kknn) #weighted k-nearest neighbors
library(e1071) #SVM
library(caret) #select tuning parameters
library(MASS) # contains the data
library(reshape2) #assist in creating boxplots
library(ggplot2) #create boxplots
library(kernlab) #assist with SVM feature selection
library(pROC)
library(class) #k-nearest neighbors
library(kknn) #weighted k-nearest neighbors
install.packages("kknn")
library(kknn) #weighted k-nearest neighbors
library(e1071) #SVM
library(caret) #select tuning parameters
library(MASS) # contains the data
library(reshape2) #assist in creating boxplots
library(ggplot2) #create boxplots
library(kernlab) #assist with SVM feature selection
library(pROC)
install.packages("pROC")
library(pROC)
set.seed(7); #replicable across our PCs?
sahdd <-
read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv',
as.is = T);
sahdd$chd <- as.factor(sahdd$chd)
SS <- cbind(sahdd[,c(1,11)], scale(sahdd[,c(-1,-6,-11)]) )
str(SS)
View(ss)
View(SS)
inTrain <- createDataPartition(y = SS$chd, # y = grouping variable,
stratified random split
p = .70, ## The proportion of records in the training set
list = FALSE)
train <- SS[inTrain,]
test <- SS[-inTrain,]
# ERRORS - book code, demo code from e1071 will not run
#linear.tune = tune.svm(chd~., data=train, kernel="linear",
cost=c(0.001, 0.01, 0.1, 1,5,10))
#poly.tune = tune.svm(chd~., data=train, kernel="polynomial",
degree=c(3,4,5), coef0=c(0.1,0.5,1,2,3,4))
SS.ksvm <- ksvm(chd ~ .,
data=train,
kernel="rbfdot",
prob.model=TRUE )
SS.ksvm
SS.ksvm <- ksvm(chd ~ .,
data=train,
kernel="rbfdot",
prob.model=TRUE )
inTrain <- createDataPartition(y = SS$chd, # y = grouping variable,
stratified random split
p = .70, ## The proportion of records in the training set
list = FALSE)
inTrain <- createDataPartition(y = SS$chd,
p = .70,
list = FALSE)
train <- SS[inTrain,]
test <- SS[-inTrain,]
SS.ksvm <- ksvm(chd ~ .,
data=train,
kernel="rbfdot",
prob.model=TRUE )
SS.ksvm
confusionMatrix(test$chd, predict(SS.ksvm, test) )
setwd("C:/Users/Andrew/Desktop/blog_2")
library(devtools)
library(blogdown)
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
library(Lahman)
library(dplyr)
library(ggplot2)
library(ggiraph)
df <- Teams%>%
filter(yearID == 1980)%>%
select(name , HR)%>%
arrange(HR)
df <- Master%>%
filter(!is.na(weight))%>%
select(weight)
df <- Master%>%
filter(!is.na(weight))%>%
select(weight)
df <- Master%>%
filter(!is.na(weight))%>%
select(weight)
head(Master)
Master%>%
filter(!is.na(weight))%>%
select(weight)
library(dplyr)
df <- Master%>%
filter(!is.na(weight))%>%
select(weight)
###Packages Needed###
library(Lahman)
library(dplyr)
library(ggplot2)
library(ggiraph)
df <- Master%>%
filter(!is.na(weight))%>%
select(weight)
df <- Master%>%
select(weight)
Master%>%
select(weight)%>%
filter(!is.na(weight))
